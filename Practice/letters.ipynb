{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import csv\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Reading data file\n",
    "raw_data = pd.read_csv('letter-recognition.data', sep=',',delimiter=None, header=None)\n",
    "\n",
    "#Processing data as X_all and Y_all\n",
    "X_all = raw_data.iloc[:,1:16]\n",
    "Y_all = raw_data.iloc[:,0]\n",
    "\n",
    "#Converting Y_all to hot vectors\n",
    "converter = lambda x: ord(x)-ord('A')\n",
    "Y_all = list(map(converter,Y_all.values.T.tolist()))\n",
    "Y_all = pd.get_dummies(Y_all).values\n",
    "\n",
    "\n",
    "X_all = pd.DataFrame(X_all)\n",
    "Y_all = pd.DataFrame(Y_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(X_all.shape)\n",
    "print(Y_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "NUM_CLASSES = Y_all.shape[1]\n",
    "INPUT_SIZE = X_all.shape[1]\n",
    "BATCH_SIZE = 10000\n",
    "NR_STEP = int(X_all.shape[0]/BATCH_SIZE)\n",
    "NR_EPOCH = 1000\n",
    "\n",
    "layer_info = [INPUT_SIZE, 20, 30, 40, NUM_CLASSES]\n",
    "layer_size = len(layer_info)\n",
    "\n",
    "logs_path = '/tf_log/'\n",
    "\n",
    "print(layer_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Inference graph construction\n",
    "\n",
    "\"\"\"The choice of standard deviation, refere following paper:\n",
    "   https://github.com/bulletcross/ML-paper-collection/blob/master/Supervised/Backprop%20thumb%20rule.pdf\"\"\"\n",
    "\n",
    "def inference_graph(X, layer_info):\n",
    "    #Define weight, bias and output under different namescope\n",
    "    layer_input = X\n",
    "    for i in range(1,len(layer_info)-1):\n",
    "        with tf.name_scope('layer_'+str(i)):\n",
    "            W = tf.Variable(tf.truncated_normal([layer_info[i-1],layer_info[i]], \n",
    "                                                stddev=1.0/math.sqrt(float(layer_info[i-1]))),\n",
    "                           name = 'weight_'+str(i))\n",
    "            tf.summary.histogram('weight_histogram_'+str(i), W)\n",
    "            b = tf.Variable(tf.zeros([layer_info[i]]),\n",
    "                           name = 'bias_'+str(i))\n",
    "            tf.summary.histogram('bias_histogram_'+str(i), b)\n",
    "            layer_output = tf.nn.relu(tf.matmul(layer_input, W) + b)\n",
    "            \n",
    "            #print(layer_output)\n",
    "            \n",
    "            layer_input = layer_output\n",
    "    \n",
    "    nr_layer = len(layer_info)\n",
    "    with tf.name_scope('layer_'+str(nr_layer-1)):\n",
    "        W = tf.Variable(tf.truncated_normal([layer_info[nr_layer-2],layer_info[nr_layer-1]], \n",
    "                                            stddev=1.0/math.sqrt(float(layer_info[nr_layer-2]))),\n",
    "                       name = 'weight_'+str(nr_layer-1))\n",
    "        tf.summary.histogram('weight_histogram_'+str(nr_layer-1), W)\n",
    "        b = tf.Variable(tf.zeros([layer_info[nr_layer-1]]),\n",
    "                        name = 'bias_'+str(nr_layer-1))\n",
    "        tf.summary.histogram('bias_histogram_'+str(nr_layer-1), b)\n",
    "        layer_output = tf.matmul(layer_input, W) + b\n",
    "\n",
    "    return layer_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training graph construction\n",
    "\n",
    "def train_graph(logit, Y, learning_rate):\n",
    "    print(logit.shape)\n",
    "    with tf.name_scope('cross_entropy'):\n",
    "        softmax_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=Y, logits=logit, name = 'xentropy')\n",
    "        loss = tf.reduce_mean(softmax_entropy, name = 'loss')\n",
    "        tf.summary.scalar('loss', loss)\n",
    "    \n",
    "    with tf.name_scope('train'):\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "        train_op = optimizer.minimize(loss)\n",
    "        \n",
    "    with tf.name_scope('accuracy'):\n",
    "        nr_correct = tf.equal(tf.argmax(Y,1), tf.argmax(logits,1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(nr_correct, tf.float32))\n",
    "        tf.summary.scalar('accuracy_mean', accuracy)\n",
    "    \n",
    "    return loss, train_op, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Tensorflow full graph construction\n",
    "\n",
    "model_graph = tf.Graph()\n",
    "with model_graph.as_default():\n",
    "    with tf.name_scope('input'):\n",
    "        X = tf.placeholder(tf.float32, shape = [None, INPUT_SIZE], name = 'X_INPUT')\n",
    "        Y = tf.placeholder(tf.float32, shape = [None, NUM_CLASSES], name = 'Y_INPUT')\n",
    "\n",
    "    logits = inference_graph(X, layer_info)\n",
    "    loss, train_op, accuracy = train_graph(logits, Y, 0.1)\n",
    "\n",
    "    merged_summary_op = tf.summary.merge_all()\n",
    "\n",
    "    init = tf.initialize_all_variables()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Training model\n",
    "\n",
    "with tf.Session(graph = model_graph) as sess:\n",
    "    \n",
    "    sess.run(init)\n",
    "    summary_writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n",
    "    \n",
    "    for epoch in range(0, NR_EPOCH):\n",
    "        for step in range(0, NR_STEP):\n",
    "            feed_X = X_all.iloc[step*BATCH_SIZE:(step+1)*BATCH_SIZE, :]\n",
    "            feed_Y = Y_all.iloc[step*BATCH_SIZE:(step+1)*BATCH_SIZE, :]\n",
    "            _, step_accuracy, step_loss, summary = sess.run([train_op, accuracy, loss, merged_summary_op],\n",
    "                                                  feed_dict={X:feed_X, Y:feed_Y})\n",
    "            summary_writer.add_summary(summary, epoch)\n",
    "            \n",
    "            if step % 1000 == 0:\n",
    "                print('Epoch= %d, step= %d, accuracy= %.2f loss= %.2f' % (epoch, step, step_accuracy, step_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
